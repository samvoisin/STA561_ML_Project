---
title: "Keras Model Version 1"
author: "Sam Voisin"
date: "4/10/2019"
output: pdf_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)


library(keras)
suppressMessages(library(tidyverse))

```


```{r helperfcns, echo = FALSE}

append_file_names <- function(root) {
  # append names of all files in a dir to root address
  files <- list.files(root)
  res <- paste0(root, files)
  return(res)
}

id_gest_num <- function(fn) {
  # identify the gesture number from the filename
  motif1 <- "gesture_\\d{1}"
  motif2 <- "\\d{1}"
  gsttxt <- stringr::str_extract(fn, motif1)
  gstnum <- as.integer(stringr::str_extract(gsttxt, motif2))
  return(gstnum)
}

```


```{r dataprep, echo = FALSE}

datafiles <- c()
gestures <- 6

root <- "../PCA_EMG_data_for_gestures/"
subjects <- paste0(root, list.files(root), "/")
datarefs <- purrr::flatten_chr(map(subjects, append_file_names))

ordered_gest_nums <- id_gest_num(datarefs)

# set up tensor dimensions
numobs <- length(ordered_gest_nums)
y_dims <- c(numobs, gestures)
x_dims <- c(numobs, 4280, 8) # every matrix is padded to 4280; 8 channels

# create categorical matrix of one-hot gesture labels
y_mat <- to_categorical(ordered_gest_nums)

# x data tensor
x_mat <- array(0, x_dims)

# dplyr tick for progress bar
prog <- progress_estimated(n = numobs)

for (i in 1:numobs) {
  # 2:9 to remove row names col; we should fix this
  single_gest_mat <- data.matrix(read.table(datarefs[i])[, 2:9]) 
  x_mat[i, , ] <- single_gest_mat
  prog$tick()$print()
}


```

## Computation Graph

```{r baselineNN}



SingleLayerNN <- keras_model_sequential()
SingleLayerNN %>%
  layer_dense(units = 8, input_shape = c()) %>% # input layerl; specify units and input shape; no activation defaults to linear for now
  layer_dense(units = 6, activation = "softmax") # output layer

summary(SingleLayerNN)

```



```{r}


SingleLayerNN %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)






```




```{r}




```



```{r tst}

mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- to_categorical(mnist$train$y)
x_test <- mnist$test$x
y_test <- to_categorical(mnist$test$y)



y_train <- mnist$train$y
dim(y_train)
dim(x_train)


```


```{r}

# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255




```



```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model)


```


```{r}

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

```





```{r}



history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 1, 
  validation_split = 0.2
  )


plot(history)

```


```{r}

model %>% evaluate(x_test, y_test)

```


```{r}

model %>% predict_classes(x_test)

```



















