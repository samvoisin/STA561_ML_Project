---
title: "Keras Model Version 1"
author: "Sam Voisin"
date: "4/10/2019"
output: pdf_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tensorflow)
library(keras)
library(rlist)
suppressMessages(library(tidyverse))


#install_keras()
#install_tensorflow(version = "1.12")
use_condaenv("r-tensorflow")

```


```{r helperfcns, echo = FALSE}

append_file_names <- function(root) {
  # append names of all files in a dir to root address
  files <- list.files(root)
  res <- paste0(root, files)
  return(res)
}

id_gest_num <- function(fn) {
  # identify the gesture number from the filename
  motif1 <- "gesture_\\d{1}"
  motif2 <- "\\d{1}"
  gsttxt <- stringr::str_extract(fn, motif1)
  gstnum <- as.integer(stringr::str_extract(gsttxt, motif2))
  return(gstnum)
}

# Data prep for ANN
dataprep <- function(roo_dir, n_gests, n_chan){
  datafiles <- c()
  gestures <- n_gests
  channels <- n_chan
  
  subjects <- paste0(root, list.files(root), "/")
  datarefs <- purrr::flatten_chr(map(subjects, append_file_names))
  
  ordered_gest_nums <- id_gest_num(datarefs)

  # set up tensor dimensions
  numobs <- length(ordered_gest_nums)
  y_dims <- c(numobs, gestures)
  x_dims <- c(numobs, 4280, channels) # every matrix is padded to 4280; 8 channels
  
  # create categorical matrix of one-hot gesture labels
  # this is giving me an extra col of zeros? thats why -1 is here
  y_mat <- to_categorical(ordered_gest_nums)[, -1]
  
  # x data tensor
  x_mat <- array(0, x_dims)
  
  # dplyr tick for progress bar
  prog <- progress_estimated(n = numobs)
  
  for (i in 1:numobs) {
    # 2:9 to remove row names col; we should fix this
    single_gest_mat <- data.matrix(read.table(datarefs[i])[, 2:9]) 
    x_mat[i, , ] <- single_gest_mat
    prog$tick()$print()
  }
  
  return(list(x_mat, y_mat, numobs))
}
```


```{r dataprepPCA, echo = FALSE}
root <- "../PCA_EMG_data_for_gestures/"
xy_mat <- dataprep(root, n_gests = 6, n_chan = 8)
x_mat <- xy_mat[[1]]
y_mat <- xy_mat[[2]]
numobs <- xy_mat[[3]]
rm(xy_mat)
```


#### Using All 8 Principle Components


```{r 8PCloss}

## CV Neural Net Helper Function
## Inputs:
  ## - x_dat = X 3-D matrix with channels, previously preproc with dataprep function
  ## - y_dat = Y Class Matrix previously prepoc with dataprep function
  ## - pcomp = # of principal components to use
  ## - neurons_per_layer = Units per non-activation layer in Keras
  ## - layers = Defines number of layers in NN (default = 1)
  ## - out_units = # of Neurons/units in activation layer of Keras NN
  ## - out_met = VECTOR of with string of output metrics
  ## - cv_folds = number folds for k-fold CV (default = 5)
  ## - verb = decide to have or not a verbose output for the NN fit (default = 0)

## Output: history object with acc metrics as data frame for each fold

cvANN <- function(x_dat, y_dat, 
                  pcomp,
                  neurons_per_layer,
                  layers = 1,
                  dropout = FALSE,
                  dropout_rate = 0.0,
                  out_units,
                  out_met,
                  cvfolds = 5,
                  verb = FALSE) {
  
  folds <- caret::createFolds(y_dat[,1], k = cvfolds) # Create folds for CV
  k_hist = list() # Create empty list to store acc metrics for each fold
  
  # remove non-principle components
  x_matpc <- x_dat[ , , 1:pcomp]
  
  cnt = 1
  # K-fold iterations 
  for (f in folds){
    # Split data into test/train datasets based on folds
    x_matpc.train <- x_matpc[-f,,]
    y_mat.train <- y_dat[-f,]
    x_matpc.test <- x_matpc[f,,]
    y_mat.test <-  y_dat[f,]
    
    # Reshape test data for validation
    x.test_dimspc <- dim(x_matpc.test)
    nobs.test <- x.test_dimspc[1]
    # flatten x_mat tensor into numobs by (gestures x channels)
    x_matpc.test <- array_reshape(x_matpc.test, c(nobs.test, x.test_dimspc[2] * x.test_dimspc[3]))
    
    # Reshape train data for validation
    x_dimspc <- dim(x_matpc.train)
    nobs <- x_dimspc[1]
    # flatten x_mat tensor into numobs by (gestures x channels)
    x_matpc.train <- array_reshape(x_matpc.train, c(nobs, x_dimspc[2] * x_dimspc[3]))
    
    #reset x_dims after reshape
    x_dimspc <- dim(x_matpc.train)
    
    # Initialize Keras NN
    ANN <- keras_model_sequential()
    
    # Fit first layer of NN
    ANN %>%
      layer_dense(units = neurons_per_layer, input_shape = x_dimspc[-1])
    
    # If layers >1 then add # of layers specified by user each with neurons_per_layer
    if (layers >1) {
      for (l in 1:layers) {
        ANN %>%
          layer_dense(units = neurons_per_layer)
        if (dropout) {
          ANN %>%
            layer_dropout(dropout_rate)
        }
      }
    }
    
    # Add "softmax" activation function
    ANN %>% 
      layer_dense(units = out_units, activation = "softmax") # output layer
    
    # Compile NN with ADA Delta and cross entropy loss
    ANN %>% compile(loss = 'categorical_crossentropy',
                    optimizer = optimizer_adadelta(),
                    metrics = out_met)
    
    # Generate history per epochs using training and test data pero fold
    history <- ANN %>% fit(
      x_matpc.train, y_mat.train, 
      epochs = 30, batch_size = 1, 
      validation_data = list(x_matpc.test, 
                             y_mat.test),
      verbose = verb)
    
    k_hist[[cnt]] = as_data_frame(history) # Store acc/loss values in list
    cnt = cnt + 1
  }
  
  return(k_hist) # Return list of length(folds), each entry being a dataframe
}


create_results_df = function(cv_ann_out, layers){
  master_df <- cv_ann_out[[1]] %>% filter( metric == "acc")
  n = dim(master_df)[1]
  master_df <- master_df %>% 
    mutate(cv_fold = rep(1, n))
 

  for (i in 2:5){
    tmp_df <- cv_ann_out[[i]] %>% filter(metric == "acc") %>% 
      mutate(cv_fold = rep(i, n))
    master_df = bind_rows(master_df, tmp_df)
  }
  
  master_df <- master_df %>% mutate(cv_fold = factor(cv_fold))
  
  train_mean <- master_df %>% 
    filter(data == "training") %>%
    group_by(epoch) %>% 
    summarize(training_avg = mean(value))
  
  test_mean <- master_df %>% 
    filter(data == "validation") %>% 
    group_by(epoch) %>% 
    summarize(validation_avg = mean(value))
    
  avg_master <- left_join(train_mean, test_mean) %>% 
    gather(key, vals, -epoch)
  
  saveRDS(master_df,file = paste0("CV_ANN_",layers,"_layers.rds"))
  saveRDS(avg_master, file = paste0("Avg_CV_ANN_",layers,"_layers.rds"))
}


max_layers = 5
for (m in 1:max_layers){
  system.time(tmp <- cvANN(x_dat = x_mat,
                y_dat = y_mat,
                pcomp = 4,
                neurons_per_layer = 8,
                layers = m,
                dropout = TRUE,
                dropout_rate = 0.2,
                out_units = 6,
                out_met = c("accuracy"),
                cvfolds = 5,
                verb = TRUE))
  
  create_results_df(tmp, m)
  
  
}
  


```


```{r}

test = read_rds("Avg_CV_ANN_5_layers.rds")
ggplot(test, aes(x = epoch, y = vals, color = key ), alpha = 0.8) + 
  geom_point() + 
  geom_line(alpha = 0.8) +
  theme_bw() +
  labs(Y = "Accuracy", x = "Epoch") +
  scale_color_discrete(name = "Legend", labels = c("Training Avg", "Validation Avg")) +
  theme(legend.position = "bottom")


plot_path = "../Tex/Graphics"
ggsave(file.path(plot_path,"Avg_CV_ANN_5_layers"), dpi = 150, device = "png")

```







